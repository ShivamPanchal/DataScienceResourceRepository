{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasagne basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LeNet network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVIL HACK: Disable cuDNN check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\r\n",
      "   Creating library D:/temp/theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpwvma2e/265abc51f7c376c224983485238ff1a5.lib and object D:/temp/theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpwvma2e/265abc51f7c376c224983485238ff1a5.exp\r\n",
      "\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 25.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "# Import layers and softmax from Lasagne\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "# x (input) and y (target) variables\n",
    "x_var = theano.tensor.tensor4('x')\n",
    "y_var = theano.tensor.ivector('y')\n",
    "\n",
    "# Input layer, un-constrained mini-batch size, 1 channel, 28 x 28\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=x_var)\n",
    "# First convolutional layer, 20 filters, 5x5 filter shape\n",
    "l_c1 = lasagne.layers.Conv2DLayer(l_in, num_filters=20, filter_size=(5,5))\n",
    "# Max-pooling layer, 2x2 pool size\n",
    "l_p1 = lasagne.layers.MaxPool2DLayer(l_c1, pool_size=(2,2))\n",
    "# Second convolutional layer, 50 filters, 5x5 filter shape\n",
    "l_c2 = lasagne.layers.Conv2DLayer(l_p1, num_filters=50, filter_size=(5,5))\n",
    "# Max-pooling layer, 2x2 pool size\n",
    "l_p2 = lasagne.layers.MaxPool2DLayer(l_c2, pool_size=(2,2))\n",
    "# Dense layer, 256 units\n",
    "l_d3 = lasagne.layers.DenseLayer(l_p2, num_units=256)\n",
    "# 50% dropout\n",
    "l_d3p = lasagne.layers.DropoutLayer(l_d3, p=0.5)\n",
    "# Final softmax dense layer, 10 units\n",
    "l_final = lasagne.layers.DenseLayer(l_d3p, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, parameters, updates, train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred_prob = lasagne.layers.get_output(l_final)\n",
    "train_loss = lasagne.objectives.categorical_crossentropy(train_pred_prob, y_var)\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_final, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(train_loss.mean(), params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "eval_pred = lasagne.layers.get_output(l_final, deterministic=True)\n",
    "eval_loss = lasagne.objectives.categorical_crossentropy(eval_pred, y_var)\n",
    "\n",
    "train_fn = theano.function([x_var, y_var], train_loss.sum(), updates=updates)\n",
    "eval_fn = theano.function([x_var, y_var], eval_loss.sum())\n",
    "pred_prob_fn = theano.function([x_var], eval_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 0.43908\n",
      "Epoch 1 train loss 0.12628\n",
      "Epoch 2 train loss 0.08842\n",
      "Epoch 3 train loss 0.07258\n",
      "Epoch 4 train loss 0.06142\n",
      "Epoch 5 train loss 0.05562\n",
      "Epoch 6 train loss 0.05125\n",
      "Epoch 7 train loss 0.04520\n",
      "Epoch 8 train loss 0.04101\n",
      "Epoch 9 train loss 0.03547\n",
      "Epoch 10 train loss 0.03502\n",
      "Epoch 11 train loss 0.03142\n",
      "Epoch 12 train loss 0.03073\n",
      "Epoch 13 train loss 0.02753\n",
      "Epoch 14 train loss 0.02542\n",
      "Epoch 15 train loss 0.02340\n",
      "Epoch 16 train loss 0.02281\n",
      "Epoch 17 train loss 0.02189\n",
      "Epoch 18 train loss 0.02032\n",
      "Epoch 19 train loss 0.02020\n",
      "Epoch 20 train loss 0.01725\n",
      "Epoch 21 train loss 0.01766\n",
      "Epoch 22 train loss 0.01606\n",
      "Epoch 23 train loss 0.01560\n",
      "Epoch 24 train loss 0.01537\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mnist_dataset, fuel\n",
    "\n",
    "mnist = mnist_dataset.MNISTTrainValTest()\n",
    "train_scheme = fuel.schemes.ShuffledScheme(examples=mnist.train.num_examples, batch_size=128)\n",
    "# Use `DataStream.default_stream`, otherwise the default transformers defined by the dataset *wont* be applied\n",
    "train_stream = fuel.streams.DataStream.default_stream(dataset=mnist.train, iteration_scheme=train_scheme)\n",
    "\n",
    "for epoch in range(25):\n",
    "    tr_loss = 0.0\n",
    "    train_stream.reset()\n",
    "    for b_x, b_y in train_stream.get_epoch_iterator():\n",
    "        tr_loss += train_fn(b_x, b_y[:,0])\n",
    "    tr_loss /= float(mnist.train.num_examples)\n",
    "    print('Epoch {} train loss {:.5f}'.format(epoch, tr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final err 0.60%\n"
     ]
    }
   ],
   "source": [
    "test_scheme = fuel.schemes.SequentialScheme(examples=mnist.test.num_examples, batch_size=128)\n",
    "# Use `DataStream.default_stream`, otherwise the default transformers defined by the dataset *wont* be applied\n",
    "test_stream = fuel.streams.DataStream.default_stream(dataset=mnist.test, iteration_scheme=test_scheme)\n",
    "test_err = 0.0\n",
    "for b_x, b_y in test_stream.get_epoch_iterator():\n",
    "    b_pred_prob = pred_prob_fn(b_x)\n",
    "    b_pred_cls = np.argmax(b_pred_prob, axis=1)\n",
    "    test_err += (b_y[:,0] != b_pred_cls).sum()\n",
    "test_err /= float(mnist.test.num_examples)\n",
    "print('Final err {:.2%}'.format(test_err))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
