{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 160.  170.]\n",
      " [ 510.  545.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 25.0% of memory, cuDNN 5004)\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import theano and theano.tensor\n",
    "import theano, theano.tensor as T\n",
    "# Import Lasagne floatX utility function for ensuring that data is of the correct type\n",
    "from lasagne.utils import floatX\n",
    "\n",
    "# Create some test data to play with\n",
    "a = floatX(np.arange(10.0).reshape((2,5)))\n",
    "b = floatX(np.arange(10.0,20.0).reshape((5,2)))\n",
    "\n",
    "# Load the data into Theano as shared data (note the optional name parameter)\n",
    "a_t = theano.shared(a, name='a')\n",
    "b_t = theano.shared(b)\n",
    "\n",
    "# Use the Theano equivalent of numpy.dot() to get the dot product\n",
    "ab_t = T.dot(a_t, b_t)\n",
    "\n",
    "# Evaluate to get the result\n",
    "print(ab_t.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and modifying shared data\n",
    "Lets use `set_value` to change the values of the shared data and re-evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92961609  0.31637555  0.18391882  0.20456028  0.567725  ]\n",
      " [ 0.5955447   0.96451449  0.65317708  0.74890661  0.65356988]]\n",
      "[[ 0.74771482  0.96130675]\n",
      " [ 0.0083883   0.10644437]\n",
      " [ 0.2987037   0.65641117]\n",
      " [ 0.80981255  0.87217593]\n",
      " [ 0.96464759  0.72368532]]\n",
      "[[ 1.46598887  1.63731575]\n",
      " [ 1.8854332   2.23007822]]\n"
     ]
    }
   ],
   "source": [
    "# Seeded random number generator\n",
    "rng = np.random.RandomState(12345)\n",
    "\n",
    "# Change the values of the shared data\n",
    "a_t.set_value(floatX(rng.uniform(low=0, high=1, size=(2,5))))\n",
    "b_t.set_value(floatX(rng.uniform(low=0, high=1, size=(5,2))))\n",
    "\n",
    "# Print the new values then evaluate the previously built expression and print:\n",
    "print(a_t.get_value())\n",
    "print(b_t.get_value())\n",
    "print(ab_t.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74771482  0.96130675  0.0083883   0.10644437  0.2987037 ]\n",
      " [ 0.65641117  0.80981255  0.87217593  0.96464759  0.72368532]\n",
      " [ 0.64247531  0.7174536   0.467599    0.32558468  0.4396446 ]\n",
      " [ 0.72968906  0.99401456  0.67687368  0.79082251  0.17091426]\n",
      " [ 0.02684928  0.80037022  0.90372252  0.02467621  0.49174732]\n",
      " [ 0.52625519  0.59636599  0.05195754  0.89508951  0.72826618]\n",
      " [ 0.81835002  0.50022274  0.81018943  0.09596852  0.21895005]\n",
      " [ 0.25871906  0.46810576  0.45937321  0.70950979  0.17805301]\n",
      " [ 0.53144991  0.16774222  0.76881391  0.92817056  0.60949367]\n",
      " [ 0.1501835   0.48962671  0.37734497  0.8486014   0.91109723]]\n",
      "[[ 0.13432555  0.76851845]\n",
      " [ 2.81232309  3.2601366 ]\n",
      " [ 1.43153954  1.84846342]\n",
      " [ 1.71037161  2.01665854]\n",
      " [ 2.29757953  2.00904632]\n",
      " [ 1.02733064  1.85243356]\n",
      " [ 1.953511    2.15374279]\n",
      " [ 1.32819879  1.44413781]\n",
      " [ 2.62176204  3.00520468]\n",
      " [ 1.93041217  2.39455056]]\n"
     ]
    }
   ],
   "source": [
    "# Seeded random number generator\n",
    "rng = np.random.RandomState(12345)\n",
    "\n",
    "# Create a 2D variable; use the matrix constructor\n",
    "x = T.matrix('x')\n",
    "\n",
    "# Weights and biases of 5-channel input, 2-channel output\n",
    "# linear model as shared data\n",
    "W = theano.shared(floatX(rng.normal(0.25, size=(5,2))))\n",
    "b = theano.shared(floatX(np.zeros((2,))))\n",
    "\n",
    "# Linear model\n",
    "linear = T.dot(x, W) + b[None,:]\n",
    "\n",
    "# Evaluation function\n",
    "eval_linear = theano.function([x], linear)\n",
    "\n",
    "# Test it:\n",
    "data_x = floatX(rng.uniform(0, 1, (10,5)))\n",
    "print(data_x)\n",
    "print(eval_linear(data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:22: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.36257768 -0.65531474]\n",
      " [-0.78711748  0.8707363 ]\n",
      " [-1.19494736 -0.0079437 ]\n",
      " [-0.76314706 -0.38118801]\n",
      " [-0.83168006  0.08415516]\n",
      " [-0.35930121  1.19500184]\n",
      " [-1.72171664 -0.53175819]\n",
      " [ 0.10672021  0.41907474]\n",
      " [-0.59528166  1.30471754]\n",
      " [-0.10103182  1.80680192]]\n",
      "2.15233039856 2.07839131355 2.00809979439 1.94127118587 1.87773060799 1.8173122406 1.75985753536 1.70521771908 1.65324997902 1.60381948948 1.55679774284 1.51206362247 1.46950137615 1.42900133133 1.39045929909 1.35377669334 1.31885933876 1.28561854362 1.25396943092 1.22383141518 1.1951290369 1.16778922081 1.14174389839 1.11692714691 1.09327757359 1.0707359314 1.04924654961 1.0287566185 1.00921571255 0.990575969219 0.97279214859 0.955821037292 0.939621925354 0.924156010151 0.909386336803 0.89527797699 0.881797671318 0.868913948536 0.856596767902 0.844817817211 0.833550155163 0.822768032551 0.812447190285 0.802564501762 0.793098092079 0.784027040005 0.77533185482 0.76699334383 0.758994102478 0.751316964626 0.743946075439 0.736865878105 0.730062186718 0.723521173 0.71722972393 0.711175620556 0.705346941948 0.699732542038 0.694321870804 0.689104914665 0.68407189846 0.679213881493 0.674522340298 0.669988930225 0.66560614109 0.661366164684 0.657262444496 0.653288066387 0.649436831474 0.645702719688 0.642079949379 0.638563275337 0.635147452354 0.63182759285 0.628599226475 0.625457763672 0.62239921093 0.619419455528 0.616514861584 0.613681793213 0.610916793346 0.608216822147 0.60557872057 0.602999389172 0.600476443768 0.598007082939 0.595588803291 0.593219161034 0.590896070004 0.588617444038 0.586381077766 0.584185063839 0.582027673721 0.579907238483 0.577821910381 0.575770258904 0.573750853539 0.571762144566 0.569802880287 0.567871689796 0.56596750021 0.564089119434 0.562235355377 0.560405313969 0.558597922325 0.556812286377 0.555047512054 0.553302764893 0.551577210426 0.549870014191 0.548180580139 0.546508312225 0.544852256775 0.54321205616 0.541586995125 0.539976477623 0.538380086422 0.53679728508 0.535227537155 0.53367036581 0.532125413418 0.530592262745 0.529070436954 0.527559638023 0.526059508324 0.524569630623 0.523089766502 0.521619617939 0.5201587677 0.518707036972 0.517264187336 0.515829920769 0.514403939247 0.512986183167 0.511576294899 0.510174095631 0.508779346943 0.507391989231 0.506011724472 0.504638433456 0.50327193737 0.501912117004 0.50055873394 0.499211788177 0.497871011496 0.49653634429 0.495207697153 0.493884891272 0.492567837238 0.491256415844 0.489950507879 0.488650143147 0.487355053425 0.486065208912 0.484780609608 0.483501046896 0.482226550579 0.480956971645 0.479692280293 0.478432416916 0.477177292109 0.475926786661 0.474681049585 0.473439782858 0.472203016281 0.470970720053 0.469742834568 0.468519359827 0.467300117016 0.466085255146 0.464874505997 0.463668018579 0.46246561408 0.461267381907 0.460073173046 0.458882987499 0.457696855068 0.456514656544 0.455336481333 0.454162031412 0.452991575003 0.451824963093 0.450662195683 0.44950312376 0.448347955942 0.447196394205 0.446048587561 0.444904506207 0.443764060736 0.442627251148 0.441494077444 0.440364539623 0.439238548279 0.438116073608 0.436997234821 0.435881912708 0.434770017862 0.43366163969 0.432556718588 0.431455224752 0.430357277393 0.429262638092 0.428171396255 0.427083581686 0.425999164581 0.42491799593 0.423840194941 0.422765821218 0.421694666147 0.420626819134 0.419562250376 0.418500900269 0.417442858219 0.416388094425 0.41533651948 0.414288133383 0.413242906332 0.412200927734 0.411162137985 0.410126507282 0.409093946218 0.408064544201 0.407038301229 0.406015217304 0.404995262623 0.403978258371 0.402964442968 0.4019536376 0.400945901871 0.399941205978 0.398939579725 0.397940963507 0.396945387125 0.395952761173 0.394963115454 0.393976539373 0.392992824316 0.392012178898 0.391034424305 0.390059649944 0.389087766409 0.388118863106 0.387152820826 0.386189639568 0.385229468346 0.384272068739 0.383317619562 0.382366001606 0.381417274475 0.38047131896 0.379528224468 0.378588020802 0.377650558949 0.376715898514 0.375784099102 0.374854981899 0.373928725719 0.373005270958 0.372084528208 0.371166527271 0.370251327753 0.369338810444 0.368429034948 0.367521941662 0.366617590189 0.36571598053 0.364816993475 0.363920748234 0.363027095795 0.362136185169 0.361247897148 0.360362261534 0.359479248524 0.35859888792 0.357721149921 0.356846004725 0.355973482132 0.355103552341 0.354236274958 0.353371500969 0.352509319782 0.351649701595 0.35079267621 0.349938124418 0.349086135626 0.348236739635 0.34738984704 0.346545428038 0.345703572035 0.344864189625 0.34402731061 0.343192875385 0.342360973358 0.341531515121 0.340704500675 0.339879959822 0.339057862759 0.338238179684 0.337420940399 0.336606144905 0.335793763399 0.334983766079 0.334176242352 0.333371073008 0.332568228245 0.33176779747 0.330969721079 0.330174058676 0.329380691051 0.328589707613 0.327801078558 0.327014744282 0.326230764389 0.325449079275 0.324669718742 0.323892682791 0.323117911816 0.322345435619 0.321575254202 0.320807337761 0.320041686296 0.319278329611 0.318517178297 0.317758321762 0.317001670599 0.31624725461 0.315495073795 0.314745098352 0.313997328281 0.313251793385 0.312508434057 0.311767280102 0.311028331518 0.310291498899 0.309556812048 0.308824390173 0.308094114065 0.30736592412 0.306639909744 0.305916041136 0.305194288492 0.304474681616 0.303757160902 0.303041756153 0.302328467369 0.30161729455 0.300908148289 0.300201117992 0.299496203661 0.298793315887 0.298092514277 0.297393739223 0.296697050333 0.296002388 0.295309782028 0.294619172812 0.293930619955 0.293244063854 0.292559564114 0.291877031326 0.291196495295 0.290517985821 0.289841473103 0.289166897535 0.288494318724 0.287823706865 0.28715506196 0.286488354206 0.285823643208 0.28516086936 0.284499973059 0.283841073513 0.283184081316 0.282528966665 0.28187584877 0.281224578619 0.280575215816 0.279927790165 0.27928224206 0.278638571501 0.277996748686 0.277356863022 0.2767187953 0.276082605124 0.275448262691 0.274815827608 0.274185180664 0.273556381464 0.272929370403 0.272304236889 0.271680951118 0.271059453487 0.270439714193 0.269821852446 0.269205749035 0.268591463566 0.267978966236 0.267368227243 0.266759306192 0.266152113676 0.265546709299 0.264943063259 0.264341175556 0.263740986586 0.263142585754 0.262545913458 0.261951029301 0.261357784271 0.260766327381 0.260176539421 0.259588479996 0.259002119303 0.258417457342 0.257834523916 0.25725325942 0.256673663855 0.256095737219 0.255519539118 0.254944980145 0.254372090101 0.253800868988 0.253231287003 0.252663314342 0.252097040415 0.251532405615 0.25096938014 0.250407993793 0.249848216772 0.249290078878 0.248733565211 0.248178631067 0.247625306249 0.247073560953 0.246523424983 0.245974868536 0.245427891612 0.244882538915 0.244338706136 0.243796497583 0.243255764246 0.242716670036 0.242179080844 0.241643071175 0.241108611226 0.240575641394 0.240044265985 0.239514380693 0.238986045122 0.238459199667 0.237933903933 0.237410068512 0.236887782812 0.236366987228 0.235847711563 0.235329896212 0.234813600779 0.234298750758 0.233785390854 0.233273506165 0.23276309669 0.23225414753 0.231746643782 0.231240600348 0.230736017227 0.230232879519 0.229731202126 0.229230955243 0.228732138872 0.22823472321 0.227738767862 0.227244228125 0.226751059294 0.226259320974 0.225768998265 0.225280091166 0.224792569876 0.224306464195 0.22382171452 0.223338365555 0.222856372595 0.222375780344 0.221896573901 0.221418708563 0.220942214131 0.220467090607 0.219993323088 0.219520896673 0.219049841166 0.21858009696 0.21811170876 0.217644646764 0.217178910971 0.216714501381 0.216251417994 0.21578964591 0.215329214931 0.214870020747 0.214412212372 0.213955670595 0.21350042522 0.213046476245 0.212593823671 0.212142467499 0.211692377925 0.211243584752 0.210796028376 0.210349753499 0.209904715419 0.209461003542 0.20901851356 0.208577305079 0.208137303591 0.207698583603 0.20726108551 0.206824824214 0.206389814615 0.205956026912 0.205523461103 0.20509211719 0.204662010074 0.204233139753 0.203805446625 0.20337896049 0.202953696251 0.202529624104 0.202106758952 0.201685070992 0.201264545321 0.200845241547 0.200427144766 0.200010210276 0.199594438076 0.199179843068 0.19876639545 0.198354125023 0.197943016887 0.197533085942 0.197124272585 0.196716621518 0.196310132742 0.195904761553 0.195500567555 0.195097476244 0.194695547223 0.194294720888 0.193895027041 0.193496450782 0.193099021912 0.192702680826 0.192307442427 0.191913321614 0.19152033329 0.19112841785 0.190737605095 0.190347880125 0.189959257841 0.189571708441 0.189185246825 0.188799887896 0.18841560185 0.188032373786 0.187650218606 0.187269151211 0.186889126897 0.186510175467 0.186132267118 0.185755416751 0.185379654169 0.185004875064 0.184631183743 0.184258535504 0.183886915445 0.183516323566 0.18314678967 0.182778269053 0.182410761714 0.182044297457 0.181678846478 0.18131442368 0.180950984359 0.180588573217 0.180227190256 0.179866790771 0.179507374763 0.179148986936 0.178791582584 0.178435146809 0.178079754114 0.177725315094 0.177371844649 0.177019372582 0.176667883992 0.176317363977 0.175967812538 0.175619214773 0.175271585584 0.17492492497 0.17457921803 0.174234479666 0.173890680075 0.17354786396 0.173205956817 0.172865003347 0.172524988651 0.172185927629 0.17184779048 0.171510577202 0.171174317598 0.170838981867 0.170504570007 0.170171052217 0.169838473201 0.169506818056 0.169176071882 0.16884624958 0.168517291546 0.168189257383 0.167862147093 0.167535915971 0.167210608721 0.166886150837 0.166562601924 0.166239947081 0.165918171406 0.165597274899 0.165277272463 0.164958149195 0.164639875293 0.164322495461 0.164005964994 0.163690313697 0.163375541568 0.163061618805 0.162748545408 0.162436336279 0.162124961615 0.16181448102 0.161504819989 0.161195993423 0.160888046026 0.160580903292 0.160274595022 0.159969165921 0.159664541483 0.159360736609 0.1590577811 0.158755630255 0.158454328775 0.158153831959 0.157854124904 0.157555267215 0.15725722909 0.156959980726 0.156663566828 0.15636792779 0.156073108315 0.155779078603 0.155485838652 0.155193403363 0.154901772738 0.154610931873 0.15432086587 0.154031589627 0.153743118048 0.153455406427 0.153168484569 0.15288233757 0.15259693563 0.152312353253 0.152028515935 0.151745438576 0.151463121176 0.151181578636 0.150900796056 0.150620758533 0.15034148097 0.150062963367 0.149785190821 0.149508178234 0.149231895804 0.148956373334 0.148681566119 0.148407533765 0.148134216666 0.147861644626 0.147589787841 0.147318691015 0.147048309445 0.14677862823 0.146509692073 0.146241471171 0.145973995328 0.145707219839 0.145441159606 0.145175784826 0.144911140203 0.144647240639 0.144383996725 0.144121497869 0.143859669566 0.143598541617 0.143338128924 0.143078401685 0.1428193748 0.142561033368 0.14230337739 0.142046421766 0.141790151596 0.141534537077 0.141279637814 0.141025424004 0.140771865845 0.140518963337 0.140266761184 0.140015229583 0.139764353633 0.139514163136 0.139264628291 0.139015749097 0.138767540455 0.138519972563 0.138273075223 0.138026848435 0.137781247497 0.137536302209 0.137292012572 0.137048378587 0.13680537045 0.136563003063 0.136321306229 0.136080235243 0.135839775205 0.135599970818 0.13536080718 0.135122269392 0.134884372354 0.134647086263 0.134410440922 0.134174406528 0.133938983083 0.133704215288 0.133470028639 0.133236482739 0.133003547788 0.132771223783 0.132539495826 0.132308378816 0.132077902555 0.131848007441 0.131618708372 0.131390020251 0.131161928177 0.130934432149 0.130707532167 0.130481243134 0.130255535245 0.1300303936 0.129805892706 0.129581928253 0.129358574748 0.129135817289 0.128913611174 0.128692001104 0.12847097218 0.128250524402 0.128030657768 0.127811342478 0.127592608333 0.127374440432 0.127156838775 0.126939803362 0.126723334193 0.126507431269 0.126292079687 0.126077309251 0.125863075256 0.125649407506 0.125436291099 0.125223726034 0.125011727214 0.124800249934 0.124589346349 0.124378979206 0.124169155955 0.123959876597 0.123751141131 0.123542949557 0.123335286975 0.123128160834 0.122921600938 0.122715547681 0.122510030866 0.122305035591 0.122100569308 0.121896624565 0.121693231165 0.121490336955 0.121287964284 0.121086135507 0.120884798467 0.12068400532 0.120483711362 0.120283946395 0.120084665716 0.119885921478 0.11968767643 0.119489952922 0.119292736053 0.119096018374 0.118899799883 0.118704080582 0.118508860469 0.118314161897 0.118119940162 0.117926202714 0.117733001709 0.11754026264 0.117348000407 0.117156282067 0.116965010762 0.116774238646 0.116583943367 0.116394162178 0.116204835474 0.11601600796 0.115827657282 0.115639783442 0.115452371538 0.115265443921 0.115079000592 0.114893019199 0.114707514644 0.114522479475 0.114337921143 0.114153839648 0.113970182836 0.113787032664 0.113604322076 0.113422080874 0.113240286708 0.11305898428 0.112878128886 0.112697705626 0.112517759204 0.112338259816 0.112159214914 0.111980631948 0.111802481115 0.111624777317 0.111447535455 0.111270710826 0.111094377935 0.110918462276 0.110742986202 0.110567964613 0.110393367708 0.110219202936 0.110045477748 0.109872199595 0.109699353576 0.109526947141 0.109354950488 0.109183393419 0.109012268484 0.108841560781 0.108671307564 0.108501449227 0.108332015574 0.108163014054 0.107994437218 0.107826277614 0.107658542693 0.107491232455 0.107324324548 0.107157826424 0.106991752982 0.106826089323 0.106660842896 0.1064959988 0.106331564486 0.106167532504 0.106003925204 0.105840705335 0.105677880347 0.105515480042 0.105353489518 0.105191886425 0.105030678213 0.104869864881 0.104709461331 0.104549452662 0.104389831424 0.104230597615 0.104071773589 0.103913329542 0.103755280375 0.103597618639 0.103440344334 0.10328347981 0.103126958013 0.102970853448 0.102815106511 0.102659761906 0.102504789829 0.102350212634 0.102195978165 0.102042153478 0.101888693869 0.10173561424 0.10158289969 0.101430557668 0.101278588176 0.101126991212 0.100975774229 0.100824914873 0.100674435496 0.100524306297 0.100374564528 0.100225187838 0.100076153874 0.0999274849892 0.0997791960835 0.0996312499046 0.0994836837053 0.0993364602327 0.099189594388 0.0990430563688 0.098896920681 0.0987511277199 0.0986056700349 0.0984605774283 0.0983158200979 0.0981714427471 0.0980273932219 0.0978836938739 0.0977403372526 0.0975973382592 0.0974546819925 0.0973123535514 0.0971703827381 0.097028747201 0.0968874618411 0.0967464968562 0.0966058820486 0.0964656174183 0.0963256806135 0.0961860865355 0.0960468053818 0.095907881856 0.0957692712545 0.0956310033798 0.0954930633307 0.095355451107 0.0952181816101 0.0950812324882 0.0949446111917 0.0948083028197 0.094672344625 0.0945366993546 0.0944013744593 0.0942663699389 0.094131693244 0.0939973369241 0.093863286078 0.0937295556068 0.0935961604118 0.0934630706906 0.0933302938938 0.0931978225708 0.0930656939745 0.0929338559508 0.0928023532033 0.0926711410284 0.0925402343273 0.0924096703529 0.0922793895006 0.0921494141221 0.0920197442174 0.0918903946877 0.09176132828 0.0916325896978 0.0915041416883 0.0913759991527 0.0912481546402 0.0911206156015 0.0909933745861 0.0908664464951 0.0907397940755 0.0906134471297 0.0904873907566 0.0903616398573 0.0902361869812 0.0901110246778 0.0899861454964 0.0898615568876 0.0897372588515 0.0896132588387 0.089489556849 0.0893661379814 0.0892430022359 \n",
      "[[-1.10053909 -0.48579779]\n",
      " [-0.64753932  1.01595485]\n",
      " [-0.93613595  0.09195533]\n",
      " [-0.9650802  -0.03179127]\n",
      " [-0.86231273 -0.19289555]\n",
      " [-0.46115217  1.04707658]\n",
      " [-1.19854856 -0.22904307]\n",
      " [-0.67229217  0.2154609 ]\n",
      " [-0.52238196  1.25571311]\n",
      " [-0.28219503  1.31642759]]\n",
      "[[-1.36257768 -0.65531474]\n",
      " [-0.78711748  0.8707363 ]\n",
      " [-1.19494736 -0.0079437 ]\n",
      " [-0.76314706 -0.38118801]\n",
      " [-0.83168006  0.08415516]\n",
      " [-0.35930121  1.19500184]\n",
      " [-1.72171664 -0.53175819]\n",
      " [ 0.10672021  0.41907474]\n",
      " [-0.59528166  1.30471754]\n",
      " [-0.10103182  1.80680192]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Target variable; the target value\n",
    "y = T.matrix('y')\n",
    "# Learning rate variable\n",
    "lr = T.scalar('learning_rate')\n",
    "\n",
    "# Loss expression; mean squared error\n",
    "loss = ((linear - y)**2).mean()\n",
    "\n",
    "# Gradient of loss with respect to W and b\n",
    "d_grad_d_W = T.grad(loss, wrt=W)\n",
    "d_grad_d_b = T.grad(loss, wrt=b)\n",
    "\n",
    "# Stochastic gradient descent; update x to new value\n",
    "# Represent as dictionary mapping target to new value\n",
    "updates = {W: W - d_grad_d_W * lr,\n",
    "           b: b - d_grad_d_b * lr}\n",
    "\n",
    "# Training function: given values for x and y, return\n",
    "# the loss and apply updates\n",
    "train_linear = theano.function([x, y, lr], loss, updates=updates)\n",
    "\n",
    "# Test it\n",
    "# Create target data, then print it\n",
    "# The target will be a linear combination of the components of x plus\n",
    "# some noise\n",
    "factors = rng.normal(size=(5,2))\n",
    "data_y = floatX(np.dot(data_x, factors) + rng.uniform(-0.1, 0.1, (10,2)))\n",
    "print(data_y)\n",
    "# 1000 iterations of SGD; the loss should decrease\n",
    "for i in xrange(1000):\n",
    "    l = train_linear(data_x, data_y, 0.01)\n",
    "    sys.stdout.write('{} '.format(l))\n",
    "    sys.stdout.flush()\n",
    "sys.stdout.write('\\n')\n",
    "sys.stdout.flush()\n",
    "# Evaluate the model\n",
    "print(eval_linear(data_x))\n",
    "print(data_y)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
