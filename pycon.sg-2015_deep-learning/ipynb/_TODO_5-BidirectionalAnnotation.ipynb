{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating sentences with a Bidirectional RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "#from blocks import initialization\n",
    "from blocks.bricks import Identity, Linear, Tanh, MLP, Softmax\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.recurrent import SimpleRecurrent, Bidirectional, BaseRecurrent\n",
    "from blocks.bricks.parallel import Merge\n",
    "#from blocks.bricks.parallel import Fork\n",
    "\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import INPUT, WEIGHT, OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size=3\n",
    "embedding_dim=3\n",
    "labels_size=10\n",
    "\n",
    "lookup = LookupTable(vocab_size, embedding_dim)\n",
    "\n",
    "encoder = Bidirectional(SimpleRecurrent(dim=embedding_dim, activation=Tanh()))\n",
    "\n",
    "mlp = MLP([Softmax()], [embedding_dim, labels_size],\n",
    "          weights_init=IsotropicGaussian(0.01),\n",
    "          biases_init=Constant(0))\n",
    "\n",
    "#encoder.prototype.apply.sequences\n",
    "#dir(encoder.prototype.apply.sequences)\n",
    "\n",
    "#combine = Merge(input_dims=dict(), output_dim=labels_size)\n",
    "#labelled = Softmax( encoder )\n",
    "\n",
    "\n",
    "x = tensor.imatrix('features')\n",
    "y = tensor.imatrix('targets')\n",
    "\n",
    "probs = mlp.apply(encoder.apply(lookup.apply(x)))\n",
    "\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), probs)\n",
    "\n",
    "#cg = ComputationGraph([cost])\n",
    "cg = ComputationGraph([probs])\n",
    "cg.variables\n",
    "#VariableFilter(roles=[OUTPUT])(cg.variables)\n",
    "\n",
    "#dir(cg.outputs)\n",
    "#np.shape(cg.outputs)\n",
    "\n",
    "#mlp = MLP([Softmax()], [embedding_dim*2, labels_size],\n",
    "#          weights_init=IsotropicGaussian(0.01),\n",
    "#          biases_init=Constant(0))\n",
    "#mlp.initialize()\n",
    "\n",
    "#fork = Fork([name for name in encoder.prototype.apply.sequences if name != 'mask'])\n",
    "#fork.input_dim = dimension\n",
    "#fork.output_dims = [dimension for name in fork.input_names]\n",
    "#print(fork.output_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the output layer needs to gather the two hidden layers (one from each direction) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#readout = Readout(\n",
    "#    readout_dim=labels_size,\n",
    "#    source_names=[encodetransition.apply.states[0], attention.take_glimpses.outputs[0]],\n",
    "#    emitter=SoftmaxEmitter(name=\"emitter\"),\n",
    "#    #feedback_brick=LookupFeedback(alphabet_size, dimension),\n",
    "#    name=\"readout\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to double the input we had to apply a bricks.Linear brick to x, even though $h_t=f(Vh_{t\u22121}+Wx_t+b)$ is what is usually thought of as the RNN equation. The reason why recurrent bricks work that way is it allows greater flexibility and modularity: $Wx_t$ can be replaced by a whole neural network if we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial States\n",
    "Recurrent models all have in common that their initial state has to be specified. However, in constructing our toy examples, we omitted to pass h0 when applying the recurrent brick. What happened?\n",
    "\n",
    "It turns out that recurrent bricks set that initial state to zero if it\u2019s not passed as argument, which is a good sane default in most cases, but we can just as well set it explicitly.\n",
    "\n",
    "We will modify the starting example so that it accumulates the input it receives, but starting from one instead of zero (figure above): \n",
    "\n",
    "$h_t=h_{t\u22121}+x_t, h_0=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h0 = tensor.matrix('h0')\n",
    "h = rnn.apply(inputs=x, states=h0)\n",
    "\n",
    "f = theano.function([x, h0], h)\n",
    "print(f(np.ones((3, 1, 3), dtype=theano.config.floatX),\n",
    "        np.ones((1, 3), dtype=theano.config.floatX))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate (or not)\n",
    "\n",
    "The apply method of a recurrent brick accepts an iterate argument, which defaults to ``True``.  Setting it to ``False`` causes the apply method to compute only one step in the sequence.\n",
    "\n",
    "This is very useful when you\u2019re trying to combine multiple recurrent layers in a network.\n",
    "\n",
    "Imagine you\u2019d like to build a network with two recurrent layers. The second layer accumulates the output of the first layer, while the first layer accumulates the input of the network and the output of the second layer (see figure below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here\u2019s how you can create a recurrent brick that encapsulate the two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeedbackRNN(BaseRecurrent):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(FeedbackRNN, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.first_recurrent_layer = SimpleRecurrent(\n",
    "            dim=self.dim, activation=Identity(), name='first_recurrent_layer',\n",
    "            weights_init=initialization.Identity())\n",
    "        self.second_recurrent_layer = SimpleRecurrent(\n",
    "            dim=self.dim, activation=Identity(), name='second_recurrent_layer',\n",
    "            weights_init=initialization.Identity())\n",
    "        self.children = [self.first_recurrent_layer,\n",
    "                         self.second_recurrent_layer]\n",
    "\n",
    "    @recurrent(sequences=['inputs'], contexts=[],\n",
    "               states=['first_states', 'second_states'],\n",
    "               outputs=['first_states', 'second_states'])\n",
    "    def apply(self, inputs, first_states=None, second_states=None):\n",
    "        first_h = self.first_recurrent_layer.apply(\n",
    "            inputs=inputs, states=first_states + second_states, iterate=False)\n",
    "        second_h = self.second_recurrent_layer.apply(\n",
    "            inputs=first_h, states=second_states, iterate=False)\n",
    "        return first_h, second_h\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        return (self.dim if name in ('inputs', 'first_states', 'second_states')\n",
    "                else super(FeedbackRNN, self).get_dim(name))\n",
    "\n",
    "x = tensor.tensor3('x')\n",
    "\n",
    "feedback = FeedbackRNN(dim=3)\n",
    "feedback.initialize()\n",
    "first_h, second_h = feedback.apply(inputs=x)\n",
    "\n",
    "f = theano.function([x], [first_h, second_h])\n",
    "for states in f(np.ones((3, 1, 3), dtype=theano.config.floatX)):\n",
    "    print(states) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There\u2019s a lot of things going on here!\n",
    "\n",
    "We defined a recurrent brick class called ``FeedbackRNN`` whose constructor initializes two bricks.recurrent.SimpleRecurrent bricks as its children.\n",
    "\n",
    "The class has a ``get_dim method`` whose purpose is to tell the dimensionality of each input to the brick\u2019s ``apply`` method.\n",
    "\n",
    "The core of the class resides in its ``apply`` method. The ``@recurrent`` decorator is used to specify which of the arguments to the method are sequences to iterate over, what is returned when the method is called and which of those returned values correspond to recurrent states. Its relationship with the inputs and outputs arguments to the ``@application`` decorator is as follows:\n",
    "\n",
    "*  ``outputs``, like in ``@application``, defines everything that\u2019s returned by ``apply``, including recurrent outputs \n",
    "*  ``states`` is a subset of ``outputs`` that corresponds to recurrent outputs, which means that the union of sequences and states forms what would be inputs in ``@application``\n",
    "\n",
    "Notice how no call to ``theano.scan()`` is being made.  This is because the implementation of ``apply`` is responsible for computing one time step of the recurrent application of the brick.  It takes states at time $t\u22121$ and inputs at time $t$ and produces the output for time $t$. The rest is all handled by the ``@recurrent`` decorator behind the scenes.\n",
    "\n",
    "This is why the iterate argument of the ``apply`` method is so useful: it allows to combine multiple recurrent brick applications within another ``apply`` implementation.\n",
    "\n",
    "### Tip\n",
    "\n",
    "When looking at a recurrent brick\u2019s documentation, keep in mind that the parameters to its apply method are explained in terms of a single iteration, i.e. with the assumption that ``iterate = False``.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}